---
AWSTemplateFormatVersion: "2010-09-09"

Description: |-
  Back up RDS/Aurora databases, etc. to a different AWS account and region

  github.com/sqlxpert/backup-events-aws  GPLv3  Copyright Paul Marcelin

Parameters:

  PlaceholderHelp:
    Type: String
    Default: "https://github.com/sqlxpert/backup-events-aws"

  PlaceholderSuggestedStackName:
    Type: String
    Default: "AWSBackupEvents"

  Enable:
    Type: String
    Description: >-
      Whether to enable or disable the EventBridge rules that trigger this
      tool's AWS Lambda functions
    Default: "true"
    AllowedValues:
      - "false"
      - "true"

  PlaceholderAdvancedParameters:
    Type: String
    Default: ""
    AllowedValues:
      - ""

  OrgId:
    Type: String
    Description: >-
      ID of your AWS Organizations organization (for example, "o-abcde12345").
      This allows EventBridge to trigger the second (secondary-region) copy
      inside the central backup AWS account, after the first (same-region)
      copy has completed.

  OnlyResourceAccountId:
    Type: String
    Description: >-
      Account number of the AWS account that contains protected resources;
      backups originate in this account. Leave blank if your protected
      resources are in multiple AWS accounts. In that case, less specific,
      organization-wide permissions will be used.

  CentralAccountId:
    Type: String
    Description: >-
      Account number of the central backup AWS account, to which backups
      will be copied. The first copy is stored in the same region as the
      protected resource and its original backup.

  CentralSecondaryRegion:
    Type: String
    Description: >-
      Region code (for example, "us-east-1") of the region in which the second
      copy in the central backup AWS account will be stored. This region
      should be different from the regions that contain protected resources,
      that is, no backups should originate in this region.

  NewDeleteAfterDays:
    Type: Number
    Description: >-
      How many days (from creation) to keep an original backup, after the
      first copy (to the central backup AWS account, but still in the same
      region) has completed. For incremental backups -- such as EBS volume
      snapshots (including those making up EC2 images AMIs), and RDS (but not
      Aurora) database snapshots, set this to at least the number of days
      between backups. See the "Important" warning in
      https://docs.aws.amazon.com/aws-backup/latest/devguide/metering-and-billing.html
    Default: 7

  VaultName:
    Type: String
    Description: >-
      Name (not complete ARN) of the vaults from which and to which backups
      will be copied. This tool requires vaults of the same name in all
      relevant AWS accounts and regions: in each AWS account and region that
      contains protected resources; within the central backup AWS account, in
      each of the same regions as protected resources and their original
      backups; and within the central backup AWS account, in the secondary
      region. You must create vaults; AWS Backup "Default" vaults are not
      suitable for cross-account use.

  CopyRoleName:
    Type: String
    Description: >-
      Name (not complete ARN) of the IAM role that the AWS Backup service will
      assume when copying backups. Roles are account-wide, not regional. This
      tool requires a role of the same name in each AWS account that contains
      protected resources and their original backups, as well as the central
      backup AWS account. Within the central backup account, in regions that
      contain protected resources and their original backups, you must also
      update vault policies to allow this role to "backup:CopyIntoBackupVault"
      .
      Before using "service-role/AWSBackupDefaultServiceRole" , see
      https://docs.aws.amazon.com/aws-backup/latest/devguide/iam-service-roles.html#default-service-roles
    Default: "service-role/AWSBackupDefaultServiceRole"

  CopyLambdaFnReservedConcurrentExecutions:
    Type: Number
    Description: >-
      How many backup copy requests can definitely be started at once. See
      https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved
    MinValue: 0
    Default: 2

  CopyLambdaFnMemoryMB:
    Type: Number
    Description: >-
      How many megabytes of memory to allocate to the "Copy" AWS Lambda
      function. Increase this only in case of out-of-memory errors. See
      https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html
    Default: 128

  CopyLambdaFnTimeoutSecs:
    Type: Number
    Description: >-
      How many seconds before execution of the "Copy" AWS Lambda function
      is canceled. Increase this only in case of time-out errors. See
      https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/
    Default: 60

  UniqueNamePrefix:
    Type: String
    Description: >-
      Function name prefix. Change only if you need to create multiple stacks
      from the same template, in the same AWS accounts and regions, for
      example, during a blue/green deployment.
    Default: "AWSBackupEvents"

  UpdateLifecycleLambdaFnReservedConcurrentExecutions:
    Type: Number
    Description: >-
      How many copied backups can definitely have their lifecycles updated at
      once. See
      https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved
    MinValue: 0
    Default: 2

  UpdateLifecycleLambdaFnMemoryMB:
    Type: Number
    Description: >-
      How many megabytes of memory to allocate to the "Update Lifecycle" AWS
      Lambda function. Increase this only in case of out-of-memory errors.
    Default: 128

  UpdateLifecycleLambdaFnTimeoutSecs:
    Type: Number
    Description: >-
      How many seconds before execution of the "Update Lifecycle" AWS Lambda
      function is canceled. Increase this only in case of time-out errors.
    Default: 60

  LogsRetainDays:
    Type: Number
    Description: >-
      How many days to keep CloudWatch logs from the AWS Lambda functions. See
      retentionInDays in
      http://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutRetentionPolicy.html
    Default: 7

  LogLevel:
    Type: String
    Description: >-
      Threshold for logging the activities of the AWS Lambda functions. See
      https://docs.python.org/3/library/logging.html#levels
    Default: ERROR
    AllowedValues:
      - CRITICAL
      - ERROR
      - WARNING
      - INFO
      - DEBUG
      - NOTSET

  CloudWatchLogsKmsKey:
    Type: String
    Description: >-
      If this is blank, AWS Lambda function logs receive CloudWatch Logs
      default non-KMS encryption. To use a customer-managed, multi-region KMS
      encryption key instead, specify "ACCOUNT:key/KEY_ID", where KEY_ID
      begins with "mrk-". The primary, or a replica key, must exist in the
      regions where backups originate. For the necessary key policy updates,
      see
      https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html#cmk-permissions
    Default: ""

  SqsKmsKey:
    Type: String
    Description: >-
      If this is blank, queue messages will not be encrypted. To use the
      AWS-managed key (which does not support key policy restrictions, or
      cross-region or cross-account usage), specify "alias/aws/sqs". To use a
      customer-managed, multi-region KMS key, specify "ACCOUNT:key/KEY_ID",
      where where KEY_ID begins with "mrk-". The primary, or a replica key,
      must exist in the regions where backups originate; key policy updates
      are generally necessary.
    Default: ""

Metadata:

  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: For Reference
        Parameters:
          - PlaceholderSuggestedStackName
          - PlaceholderHelp
      - Label:
          default: Essential
        Parameters:
          - Enable
          - OrgId
          - OnlyResourceAccountId
          - CentralAccountId
          - CentralSecondaryRegion
          - VaultName
          - CopyRoleName
          - NewDeleteAfterDays
      - Label:
          default: Advanced...
        Parameters:
          - PlaceholderAdvancedParameters
      - Label:
          default: >-
            AWS Lambda function to start copying a backup
        Parameters:
          - CopyLambdaFnReservedConcurrentExecutions
          - CopyLambdaFnMemoryMB
          - CopyLambdaFnTimeoutSecs
          - UniqueNamePrefix
      - Label:
          default: >-
            AWS Lambda function to schedule an original backup for deletion
        Parameters:
          - UpdateLifecycleLambdaFnReservedConcurrentExecutions
          - UpdateLifecycleLambdaFnMemoryMB
          - UpdateLifecycleLambdaFnTimeoutSecs
      - Label:
          default: Logs
        Parameters:
          - LogsRetainDays
          - LogLevel
          - CloudWatchLogsKmsKey
      - Label:
          default: >-
            "Dead letter" SQS queue for event errors
        Parameters:
          - SqsKmsKey

    ParameterLabels:

      PlaceholderSuggestedStackName:
        default: Suggested stack name
      PlaceholderHelp:
        default: For help with this stack, see

      Enable:
        default: Enabled?
      OrgId:
        default: AWS organization ID
      OnlyResourceAccountId:
        default: Only AWS account with protected resources
      CentralAccountId:
        default: Central backup AWS account
      CentralSecondaryRegion:
        default: Secondary region
      VaultName:
        default: Vault name
      CopyRoleName:
        default: IAM role name for copying backups
      NewDeleteAfterDays:
        default: Days (from creation) to keep an original backup

      PlaceholderAdvancedParameters:
        default: Do not change the parameters below, unless necessary!

      CopyLambdaFnReservedConcurrentExecutions:
        default: Number of parallel operations
      CopyLambdaFnMemoryMB:
        default: Megabytes of memory
      CopyLambdaFnTimeoutSecs:
        default: Seconds before timeout
      UniqueNamePrefix:
        default: Unique function name prefix

      UpdateLifecycleLambdaFnReservedConcurrentExecutions:
        default: Number of parallel operations
      UpdateLifecycleLambdaFnMemoryMB:
        default: Megabytes of memory
      UpdateLifecycleLambdaFnTimeoutSecs:
        default: Seconds before timeout

      LogsRetainDays:
        default: Days before deleting
      LogLevel:
        default: Message level
      CloudWatchLogsKmsKey:
        default: KMS encryption key

      SqsKmsKey:
        default: KMS encryption key

Conditions:

  EnableTrue: !Equals [ !Ref Enable, "true" ]

  OnlyResourceAccountIdBlank: !Equals [ !Ref OnlyResourceAccountId, "" ]

  InCentralAccount: !Equals [ !Ref CentralAccountId, !Ref AWS::AccountId ]
  NotInCentralAccount: !Not [ !Condition InCentralAccount ]

  InSecondaryRegion: !Equals [ !Ref CentralSecondaryRegion, !Ref AWS::Region ]
  NotInSecondaryRegion: !Not [ !Condition InSecondaryRegion ]

  InCentralAccountSecondaryRegion: !And [ !Condition InCentralAccount, !Condition InSecondaryRegion]
  NotInCentralAccountSecondaryRegion: !Not [ !Condition InCentralAccountSecondaryRegion ]

  InCentralAccountNotInSecondaryRegion: !And [ !Condition InCentralAccount, !Condition NotInSecondaryRegion]

  NotInCentralAccountNotInSecondaryRegion: !And [ !Condition NotInCentralAccount, !Condition NotInSecondaryRegion ]

  CloudWatchLogsKmsKeyBlank: !Equals [ !Ref CloudWatchLogsKmsKey, "" ]

  SqsKmsKeyBlank: !Equals [ !Ref SqsKmsKey, "" ]
  SqsKmsKeyCustom:
    Fn::And:
      - !Not [ !Condition SqsKmsKeyBlank ]
      - !Not [ !Equals [ !Ref SqsKmsKey, "alias/aws/sqs" ] ]

Resources:

  # Administrator: Restrict iam:PassRole to prevent use of these roles for
  # arbitrary AWS Lambda functions.

  EventBridgeInvokeCopyLambdaFnInCentralAcctRole:
    Type: AWS::IAM::Role
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      # Cross-account:
      RoleName: !Sub "${UniqueNamePrefix}-EventBridgeInvokeCopyLambdaFnInCentralAcctRole"

      Description: !Sub "For ${AWS::Region} region"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: { Service: events.amazonaws.com }
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaInvoke
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: lambda:InvokeFunction
                Resource: !Sub "arn:${AWS::Partition}:lambda:${AWS::Region}:${CentralAccountId}:function:${UniqueNamePrefix}-CopyLambdaFn"
                Condition:
                  StringEquals:
                    "aws:ResourceOrgID": !Ref OrgId

  EventTargetErrorQueuePol:
    Type: AWS::SQS::QueuePolicy
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Queues: [ !Ref EventTargetErrorQueue ]
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Sid: RequireSsl
            Effect: Deny
            Principal: "*"
            Action: sqs:*
            Resource: "*"
            Condition:
              Bool: { aws:SecureTransport: "false" }
          - Effect: Allow
            Principal: "*"
            Action: sqs:GetQueueAttributes
            Resource: "*"
          - Sid: DeadLetterSource
            Effect: Allow
            Principal: "*"
            Action: sqs:SendMessage
            Resource: "*"
            Condition:
              ArnEquals: { aws:SourceArn: !GetAtt Copy1ToCentralAcctCompletedCopyLambdaFnEvRule.Arn }
          - Sid: ExclusiveSource
            Effect: Deny
            Principal: "*"
            Action: sqs:SendMessage
            Resource: "*"
            Condition:
              ArnNotEquals: { aws:SourceArn: !GetAtt Copy1ToCentralAcctCompletedCopyLambdaFnEvRule.Arn }

  CopyLambdaFnRole:
    Type: AWS::IAM::Role
    Condition: NotInCentralAccountSecondaryRegion
    Properties:
      Description: !Sub "For ${AWS::Region} region"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: { Service: lambda.amazonaws.com }
            Action: sts:AssumeRole
      Policies:

        - PolicyName: CloudWatchLogsCreateLogGroupIfDeleted
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                Resource: !GetAtt CopyLambdaFnLogGrp.Arn
        - PolicyName: CloudWatchLogsWrite
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:${CopyLambdaFnLogGrp}:log-stream:*"
                # !GetAtt LogGroup.Arn ends with :* instead of allowing us to
                # append :log-stream:* to make a log stream ARN

        - PolicyName: BackupCopy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: backup:StartCopyJob
                Resource: "*"
                Condition:
                  StringEquals:
                    "aws:ResourceAccount": !Ref AWS::AccountId
                    # This ia a global ("aws:"), not AWS Backup ("backup:")
                    # condition key. "Resource" refers to the original backup
                    # or the first copy, not to the "protected resource" (that
                    # is, not to the resource that was backed up).
              - Effect: Allow
                Action: iam:PassRole
                Resource: !Sub "arn:${AWS::Partition}:iam::${AWS::AccountId}:role/${CopyRoleName}"
                Condition:
                  StringLike: { "iam:PassedToService": "backup.amazonaws.com" }

  UpdateLifecycleLambdaFnRole:
    Type: AWS::IAM::Role
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Description: !Sub "For ${AWS::Region} region"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal: { Service: lambda.amazonaws.com }
            Action: sts:AssumeRole
      Policies:

        - PolicyName: CloudWatchLogsCreateLogGroupIfDeleted
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                Resource: !GetAtt UpdateLifecycleLambdaFnLogGrp.Arn
        - PolicyName: CloudWatchLogsWrite
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:${UpdateLifecycleLambdaFnLogGrp}:log-stream:*"
                # !GetAtt LogGroup.Arn ends with :* instead of allowing us to
                # append :log-stream:* to make a log stream ARN

        - PolicyName: BackupRead
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: backup:DescribeRecoveryPoint
                Resource: "*"
                Condition:
                  StringEquals:
                    "aws:ResourceAccount": !Ref AWS::AccountId
                    # This is a global ("aws:"), not AWS Backup ("backup:")
                    # condition key. "Resource" refers to the original backup,
                    # not to the "protected resource" (that is, not to the
                    # resource that was backed up).

        - PolicyName: BackupWrite
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: backup:UpdateRecoveryPointLifecycle
                Resource: "*"
                Condition:
                  StringEquals:
                    "aws:ResourceAccount": !Ref AWS::AccountId
                    # This is a global ("aws:"), not AWS Backup ("backup:")
                    # condition key. "Resource" refers to the original backup,
                    # not to the "protected resource" (that is, not to the
                    # resource that was backed up).

  EventTargetErrorQueue:
    Type: AWS::SQS::Queue
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      DelaySeconds: 0
      KmsMasterKeyId:
        Fn::If:
          - SqsKmsKeyBlank
          - !Ref AWS::NoValue
          - Fn::If:
              - SqsKmsKeyCustom
              - !Sub "arn:${AWS::Partition}:kms:${AWS::Region}:${SqsKmsKey}"
              - !Ref SqsKmsKey
      KmsDataKeyReusePeriodSeconds:
        Fn::If:
          - SqsKmsKeyBlank
          - !Ref AWS::NoValue
          - 86400  # seconds (24 hours)
      MessageRetentionPeriod: 604800  # seconds (7 days)
      ReceiveMessageWaitTimeSeconds: 20  # long polling (lowest cost)
      VisibilityTimeout: 60  # seconds

  CopyLambdaFnLogGrp:
    Type: AWS::Logs::LogGroup
    Condition: NotInCentralAccountSecondaryRegion
    Properties:
      RetentionInDays: !Ref LogsRetainDays
      KmsKeyId:
        Fn::If:
          - CloudWatchLogsKmsKeyBlank
          - !Ref AWS::NoValue
          - !Sub "arn:${AWS::Partition}:kms:${AWS::Region}:${CloudWatchLogsKmsKey}"

  UpdateLifecycleLambdaFnLogGrp:
    Type: AWS::Logs::LogGroup
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      RetentionInDays: !Ref LogsRetainDays
      KmsKeyId:
        Fn::If:
          - CloudWatchLogsKmsKeyBlank
          - !Ref AWS::NoValue
          - !Sub "arn:${AWS::Partition}:kms:${AWS::Region}:${CloudWatchLogsKmsKey}"

  BackupCompletedCopyLambdaFnEvRule:
    Type: AWS::Events::Rule
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Description: >-
        After backup has been created, store first (same-region) copy in
        central backup AWS account
      EventPattern:
        source: [ aws.backup ]
        detail-type: [ Backup Job State Change ]
        detail:
          state: [ COMPLETED ]
          backupVaultArn:
            - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${AWS::AccountId}:backup-vault:${VaultName}"
        version: [ "0" ]
      Targets:
        - Id: !Ref CopyLambdaFn
          Arn: !GetAtt CopyLambdaFn.Arn
      State: !If [ EnableTrue, ENABLED, DISABLED ]

  Copy1ToCentralAcctCompletedCopyLambdaFnEvRule:
    Type: AWS::Events::Rule
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      # Cross-account:
      Name: !Sub "${UniqueNamePrefix}-Copy1ToCentralAcctCompletedCopyLambdaFnEvRule"

      Description: >-
        After first (same-region) copy of backup has been stored in central
        backup AWS account, store second (secondary-region) copy
      EventPattern:
        source: [ aws.backup ]
        detail-type: [ Copy Job State Change ]
        detail:
          state: [ COMPLETED ]
          sourceBackupVaultArn:
            - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${AWS::AccountId}:backup-vault:${VaultName}"
          destinationBackupVaultArn:
            - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${CentralAccountId}:backup-vault:${VaultName}"
        version: [ "0" ]
      Targets:
        - Id: !Sub "${UniqueNamePrefix}-CopyLambdaFn"
          RoleArn: !GetAtt EventBridgeInvokeCopyLambdaFnInCentralAcctRole.Arn
          Arn: !Sub "arn:${AWS::Partition}:lambda:${AWS::Region}:${CentralAccountId}:function:${UniqueNamePrefix}-CopyLambdaFn"
          DeadLetterConfig:
            Arn: !GetAtt EventTargetErrorQueue.Arn
      State: !If [ EnableTrue, ENABLED, DISABLED ]

  # Administrator: Block other invocations
  BackupCompletedCopyLambdaFnEvRulePerm:
    Type: AWS::Lambda::Permission
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref CopyLambdaFn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt BackupCompletedCopyLambdaFnEvRule.Arn

  Copy1ToCentralAcctCompletedCopyLambdaFnEvRulePerm:
    Type: AWS::Lambda::Permission
    Condition: InCentralAccountNotInSecondaryRegion
    Properties:
      SourceArn:
        Fn::If:
          - OnlyResourceAccountIdBlank
          - !Ref AWS::NoValue
          - !Sub "arn:aws:events:${AWS::Region}:${OnlyResourceAccountId}:rule/${UniqueNamePrefix}-Copy1ToCentralAcctCompletedCopyLambdaFnEvRule"
      PrincipalOrgID: !Ref OrgId
      Principal:
        Fn::If:
          - OnlyResourceAccountIdBlank
          - "*"
          - !Sub "arn:aws:iam::${OnlyResourceAccountId}:role/${UniqueNamePrefix}-EventBridgeInvokeCopyLambdaFnInCentralAcctRole"
      Action: lambda:InvokeFunction
      FunctionName: !Sub "${UniqueNamePrefix}-CopyLambdaFn"

  CopyCompletedEvRuleUpdateLifecycleLambdaFn:
    Type: AWS::Events::Rule
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Description: >-
        After first (same-region) copy has been stored in central backup AWS
        account, schedule deletion of original backup
      EventPattern:
        source: [ aws.backup ]
        detail-type: [ Copy Job State Change ]
        detail:
          state: [ COMPLETED ]
          sourceBackupVaultArn:
            - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${AWS::AccountId}:backup-vault:${VaultName}"
          destinationBackupVaultArn:
            - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${CentralAccountId}:backup-vault:${VaultName}"
        version: [ "0" ]
      Targets:
        - Id: !Ref UpdateLifecycleLambdaFn
          Arn: !GetAtt UpdateLifecycleLambdaFn.Arn
      State: !If [ EnableTrue, ENABLED, DISABLED ]

  # Administrator: Block other invocations
  UpdateLifecycleLambdaFnPerm:
    Type: AWS::Lambda::Permission
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref UpdateLifecycleLambdaFn
      Principal: events.amazonaws.com
      SourceArn: !GetAtt CopyCompletedEvRuleUpdateLifecycleLambdaFn.Arn

  CopyLambdaFn:
    Condition: NotInCentralAccountSecondaryRegion
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::If:
          - InCentralAccount
          - !Sub "${UniqueNamePrefix}-CopyLambdaFn"  # Cross-account
          - !Ref AWS::NoValue  # Same-account, prefer CloudFormation naming
      Role: !GetAtt CopyLambdaFnRole.Arn
      ReservedConcurrentExecutions: !Ref CopyLambdaFnReservedConcurrentExecutions
      Timeout: !Ref CopyLambdaFnTimeoutSecs
      MemorySize: !Ref CopyLambdaFnMemoryMB
      LoggingConfig:
        LogGroup: !Ref CopyLambdaFnLogGrp
        LogFormat: JSON
        SystemLogLevel: WARN
        ApplicationLogLevel: !Ref LogLevel
      Architectures:
        - arm64
      Runtime: python3.13
      # To avoid making users build a source bundle and distribute it to a
      # bucket in every target region (an AWS Lambda requirement when using
      # S3), supply shared, multi-handler source code in-line...
      Environment:
        Variables:
          NEW_DELETE_AFTER_DAYS: "0"  # Not needed by this handler
          COPY_ROLE_ARN: !Sub "arn:${AWS::Partition}:iam::${AWS::AccountId}:role/${CopyRoleName}"
          BACKUP_VAULT_NAME: !Ref VaultName
          DESTINATION_BACKUP_VAULT_ARN:
            Fn::If:
              - InCentralAccount
              - !Sub "arn:${AWS::Partition}:backup:${CentralSecondaryRegion}:${AWS::AccountId}:backup-vault:${VaultName}"
              - !Sub "arn:${AWS::Partition}:backup:${AWS::Region}:${CentralAccountId}:backup-vault:${VaultName}"
      Handler: index.lambda_handler_copy
      Code:
        ZipFile: |
          #!/usr/bin/env python3
          """Back up RDS/Aurora databases, etc. to a different AWS account and region

          github.com/sqlxpert/backup-events-aws  GPLv3  Copyright Paul Marcelin
          """

          import os
          import logging
          import time
          import json
          import datetime
          import botocore
          import boto3

          logger = logging.getLogger()
          # Skip "credentials in environment" INFO message, unavoidable in AWS Lambda:
          logging.getLogger("botocore").setLevel(logging.WARNING)

          os.environ["TZ"] = "UTC"
          time.tzset()
          # See get_update_lifecycle_kwargs and lambda_handler_update_lifecycle

          NEW_DELETE_AFTER_DAYS = int(os.environ["NEW_DELETE_AFTER_DAYS"])


          def get_backup_action_kwargs_base():
            """Get base kwargs for AWS Backup methods, from environment variables
            """
            backup_vault_name = os.environ["BACKUP_VAULT_NAME"]
            return {
              "DEFAULT": {"BackupVaultName": backup_vault_name},
              "start_copy_job": {
                "IamRoleArn": os.environ["COPY_ROLE_ARN"],
                "SourceBackupVaultName": backup_vault_name,
                "DestinationBackupVaultArn": os.environ["DESTINATION_BACKUP_VAULT_ARN"],
              },
            }


          def log(entry_type, entry_value, log_level=logging.INFO):
            """Emit a JSON-format log entry
            """
            entry_value_out = json.loads(json.dumps(entry_value, default=str))
            # Avoids "Object of type datetime is not JSON serializable" in
            # https://github.com/aws/aws-lambda-python-runtime-interface-client/blob/9efb462/awslambdaric/lambda_runtime_log_utils.py#L109-L135
            #
            # The JSON encoder in the AWS Lambda Python runtime isn't configured to
            # serialize datatime values in responses returned by AWS's own Python SDK!
            #
            # Alternative considered:
            # https://docs.powertools.aws.dev/lambda/python/latest/core/logger/

            logger.log(
              log_level, "", extra={"type": entry_type, "value": entry_value_out}
            )


          def boto3_success(resp):
            """Take a boto3 response, return True if result was success

            Success means an AWS operation has started, not necessarily that it has
            completed. For example, it may take hours to copy a backup.
            """
            return (
              isinstance(resp, dict)
              and isinstance(resp.get("ResponseMetadata"), dict)
              and (resp["ResponseMetadata"].get("HTTPStatusCode") == 200)
            )


          class Backup():
            """AWS Backup recovery point

            If the object is not also a subclass, then the recovery point is the
            original, from start_backup_job :
            https://docs.aws.amazon.com/aws-backup/latest/devguide/eventbridge.html#backup-job-state-change-completed
            """
            _boto3_client = None
            action_kwargs_base = get_backup_action_kwargs_base()

            _from_job_id_key = "backupJobId"

            def __init__(self, from_event):
              self._from_event = from_event

            @staticmethod
            def new(from_event):
              """Create a Backup or BackupCopy instance

              Takes a start_backup_job or start_copy_job state change COMPLETED event
              (EventBridge filters in CloudFormation allow only acceptable events)
              """
              new_object_class = (
                BackupCopy if from_event["detail-type"].startswith("Copy ") else Backup
              )
              return new_object_class(from_event)

            @classmethod
            def get_boto3_client(cls):
              """Create (if needed) and return a boto3 client for the AWS Backup service

              boto3 method references can only be resolved at run-time, against an
              instance of an AWS service's Client class.
              http://boto3.readthedocs.io/en/latest/guide/events.html#extensibility-guide

              Alternatives considered:
              https://github.com/boto/boto3/issues/3197#issue-1175578228
              https://github.com/aws-samples/boto-session-manager-project
              """
              if cls._boto3_client is None:
                cls._boto3_client = boto3.client(
                  "backup", config=botocore.config.Config(retries={"mode": "standard"})
                )
              return cls._boto3_client

            # pylint: disable=missing-function-docstring

            @property
            def _from_job_details(self):
              return self._from_event.get("detail", {})

            @property
            def from_event(self):
              return self._from_event

            @property
            def from_job_id(self):
              return self._from_job_details.get(self._from_job_id_key, "")

            @property
            def from_backup_arn(self):  # Reserve from_rsrc_arn for original backups
              return ""

            @property
            def arn(self):
              return self._from_event.get("resources", [""])[0]

            # pylint: enable=missing-function-docstring

            def valid(self):
              """Return True if all required attributes are non-empty

              A cursory validation, but EventBridge filters in CloudFormation allow only
              acceptable events, which come from AWS Backup.
              """
              return all([self.from_job_id, self.arn])  # More attributes coming!

            def log_action(self, action_name, action_kwargs, exception=None, resp=None):
              """Log the AWS Lambda event and the outcome of an action on a backup
              """
              log_level = logging.INFO if boto3_success(resp) else logging.ERROR

              log("LAMBDA_EVENT", self.from_event, log_level)
              log(f"{action_name.upper()}_KWARGS", action_kwargs, log_level)

              if exception is not None:
                log("EXCEPTION", exception, log_level)
              elif resp is not None:
                log("AWS_RESPONSE", resp, log_level)

            def do_action(
              self, action_name, kwargs_add={}, validate_backup=True
            ):  # pylint: disable=dangerous-default-value
              """Take an AWS Backup method and kwargs, log outcome, and return response
              """
              action_kwargs = self.action_kwargs_base.get(
                action_name, self.action_kwargs_base["DEFAULT"]
              ) | kwargs_add  # Copy, don't update!
              resp = None

              if validate_backup and not self.valid():
                self.log_action(action_name, action_kwargs)
              else:
                action_method = getattr(self.get_boto3_client(), action_name)
                try:
                  resp = action_method(**action_kwargs)
                except Exception as misc_exception:
                  self.log_action(action_name, action_kwargs, exception=misc_exception)
                  raise
                self.log_action(action_name, action_kwargs, resp=resp)

              return resp


          class BackupCopy(Backup):
            """AWS Backup recovery point copy, from start_copy_job

            https://docs.aws.amazon.com/aws-backup/latest/devguide/eventbridge.html#copy-job-state-change-completed

            Why didn't AWS use the same structure and keys for start_backup_job and the
            destination half of start_copy_job ? Both methods put a backup in a
            destination vault.
            """
            _from_job_id_key = "copyJobId"

            @property
            def from_backup_arn(self):  # Reserve from_rsrc_arn for original backups
              return self._from_event.get("resources", [""])[0]

            @property
            def arn(self):  # pylint: disable=missing-function-docstring
              return self._from_job_details.get("destinationRecoveryPointArn", "")


            def valid(self):
              """Return True if all required attributes are non-empty

              A cursory validation, but EventBridge filters in CloudFormation allow only
              acceptable events, which come from AWS Backup.
              """
              return all([self.from_job_id, self.from_backup_arn, self.arn])


          def get_update_lifecycle_kwargs(describe_resp):
            """Take a describe response, return update_recovery_point_lifecycle kwargs

            Sets/reduces DeleteAfterDays, so a backup that has been copied to another
            vault can be scheduled for deletion from the original vault. If the result
            dict is empty, no lifecycle update is needed.

            Warnings:
            - Before calling describe_recovery_point , use tzset to set the local time
              zone to UTC, for correct results.
            - For safety, this function works in UTC whole days, stripping time and
              leaving a whole-day margin ( +1 and strict < inequality ). AWS Backup
              measures lifecycles (MoveToColdStorageAfterDays, DeleteAfterDays) in whole
              days, but CreationDate -- misnamed -- includes a precise time, and then
              deletion occurs "at a randomly chosen point over the following 8 hours".
              https://docs.aws.amazon.com/aws-backup/latest/devguide/recov-point-create-on-demand-backup.html
            """
            kwargs_out = {}
            lifecycle = dict(describe_resp.get("Lifecycle", {}))  # Update the copy...

            creation_date = describe_resp["CreationDate"].date()
            today = datetime.date.today()
            days_old = (today - creation_date).days

            delete_after_days_minima = [days_old, 1, NEW_DELETE_AFTER_DAYS]
            delete_after_days_maximum = lifecycle.get("DeleteAfterDays")  # Don't delay

            storage_class = describe_resp.get("StorageClass")
            cold_storage_after_days = (
              lifecycle.get("MoveToColdStorageAfterDays")
              if lifecycle.get("OptInToArchiveForSupportedResources", False) else
              None
            )

            if storage_class == "DELETED":
              delete_after_days_maximum = 0
            elif cold_storage_after_days is not None:
              if (storage_class == "WARM") and (days_old < cold_storage_after_days):
                # Has not yet transitioned cold storage, and is not scheduled to, today
                lifecycle.update({
                  "OptInToArchiveForSupportedResources": False,
                  "MoveToColdStorageAfterDays": -1,
                })
              else:
                # Has already transitioned cold storage, or is scheduled to, today
                delete_after_days_minima.append(cold_storage_after_days + 90)
            elif storage_class == "COLD":
              # In case AWS Backup someday supports creation in/non-scheduled move to
              # cold storage, could have entered cold storage as late as today
              delete_after_days_minima.append(days_old + 90)

            delete_after_days = max(delete_after_days_minima) + 1
            if (
              (delete_after_days_maximum is None)
              or (delete_after_days < delete_after_days_maximum)
            ):
              lifecycle["DeleteAfterDays"] = delete_after_days
              kwargs_out = {"Lifecycle": lifecycle}

            return kwargs_out


          def lambda_handler_copy(event, context):  # pylint: disable=unused-argument
            """Copy a backup to a vault in another AWS account OR another region
            """
            backup = Backup.new(event)
            backup.do_action(
              "start_copy_job",
              {
                "RecoveryPointArn": backup.arn,
                "IdempotencyToken": backup.from_job_id,
              }
            )


          def lambda_handler_update_lifecycle(event, context):  # pylint: disable=unused-argument
            """Schedule deletion of a backup that has been copied to another vault

            Warning:
            - Before calling describe_recovery_point , use tzset to set the local time
              zone to UTC, for correct results.
            """
            backup = Backup.new(event)
            describe_resp = backup.do_action(
              "describe_recovery_point",
              {"RecoveryPointArn": backup.from_backup_arn}
            )
            if boto3_success(describe_resp):
              kwargs_add = get_update_lifecycle_kwargs(describe_resp)
              if kwargs_add:
                backup.do_action(
                  "update_recovery_point_lifecycle",
                  kwargs_add | {"RecoveryPointArn": backup.from_backup_arn},
                  validate_backup=False
                )
        # ZIPFILE_END

  UpdateLifecycleLambdaFn:
    Type: AWS::Lambda::Function
    Condition: NotInCentralAccountNotInSecondaryRegion
    Properties:
      Role: !GetAtt UpdateLifecycleLambdaFnRole.Arn
      ReservedConcurrentExecutions: !Ref UpdateLifecycleLambdaFnReservedConcurrentExecutions
      Timeout: !Ref UpdateLifecycleLambdaFnTimeoutSecs
      MemorySize: !Ref UpdateLifecycleLambdaFnMemoryMB
      LoggingConfig:
        LogGroup: !Ref UpdateLifecycleLambdaFnLogGrp
        LogFormat: JSON
        SystemLogLevel: WARN
        ApplicationLogLevel: !Ref LogLevel
      Architectures:
        - arm64
      Runtime: python3.13
      # To avoid making users build a source bundle and distribute it to a
      # bucket in every target region (an AWS Lambda requirement when using
      # S3), supply shared, multi-handler source code in-line...
      Environment:
        Variables:
          NEW_DELETE_AFTER_DAYS: !Ref NewDeleteAfterDays
          COPY_ROLE_ARN: ""  # Not needed by this handler
          BACKUP_VAULT_NAME: !Ref VaultName
          DESTINATION_BACKUP_VAULT_ARN: ""  # Not needed by this handler
      Handler: index.lambda_handler_update_lifecycle
      Code:
        ZipFile: |
          #!/usr/bin/env python3
          """Back up RDS/Aurora databases, etc. to a different AWS account and region

          github.com/sqlxpert/backup-events-aws  GPLv3  Copyright Paul Marcelin
          """

          import os
          import logging
          import time
          import json
          import datetime
          import botocore
          import boto3

          logger = logging.getLogger()
          # Skip "credentials in environment" INFO message, unavoidable in AWS Lambda:
          logging.getLogger("botocore").setLevel(logging.WARNING)

          os.environ["TZ"] = "UTC"
          time.tzset()
          # See get_update_lifecycle_kwargs and lambda_handler_update_lifecycle

          NEW_DELETE_AFTER_DAYS = int(os.environ["NEW_DELETE_AFTER_DAYS"])


          def get_backup_action_kwargs_base():
            """Get base kwargs for AWS Backup methods, from environment variables
            """
            backup_vault_name = os.environ["BACKUP_VAULT_NAME"]
            return {
              "DEFAULT": {"BackupVaultName": backup_vault_name},
              "start_copy_job": {
                "IamRoleArn": os.environ["COPY_ROLE_ARN"],
                "SourceBackupVaultName": backup_vault_name,
                "DestinationBackupVaultArn": os.environ["DESTINATION_BACKUP_VAULT_ARN"],
              },
            }


          def log(entry_type, entry_value, log_level=logging.INFO):
            """Emit a JSON-format log entry
            """
            entry_value_out = json.loads(json.dumps(entry_value, default=str))
            # Avoids "Object of type datetime is not JSON serializable" in
            # https://github.com/aws/aws-lambda-python-runtime-interface-client/blob/9efb462/awslambdaric/lambda_runtime_log_utils.py#L109-L135
            #
            # The JSON encoder in the AWS Lambda Python runtime isn't configured to
            # serialize datatime values in responses returned by AWS's own Python SDK!
            #
            # Alternative considered:
            # https://docs.powertools.aws.dev/lambda/python/latest/core/logger/

            logger.log(
              log_level, "", extra={"type": entry_type, "value": entry_value_out}
            )


          def boto3_success(resp):
            """Take a boto3 response, return True if result was success

            Success means an AWS operation has started, not necessarily that it has
            completed. For example, it may take hours to copy a backup.
            """
            return (
              isinstance(resp, dict)
              and isinstance(resp.get("ResponseMetadata"), dict)
              and (resp["ResponseMetadata"].get("HTTPStatusCode") == 200)
            )


          class Backup():
            """AWS Backup recovery point

            If the object is not also a subclass, then the recovery point is the
            original, from start_backup_job :
            https://docs.aws.amazon.com/aws-backup/latest/devguide/eventbridge.html#backup-job-state-change-completed
            """
            _boto3_client = None
            action_kwargs_base = get_backup_action_kwargs_base()

            _from_job_id_key = "backupJobId"

            def __init__(self, from_event):
              self._from_event = from_event

            @staticmethod
            def new(from_event):
              """Create a Backup or BackupCopy instance

              Takes a start_backup_job or start_copy_job state change COMPLETED event
              (EventBridge filters in CloudFormation allow only acceptable events)
              """
              new_object_class = (
                BackupCopy if from_event["detail-type"].startswith("Copy ") else Backup
              )
              return new_object_class(from_event)

            @classmethod
            def get_boto3_client(cls):
              """Create (if needed) and return a boto3 client for the AWS Backup service

              boto3 method references can only be resolved at run-time, against an
              instance of an AWS service's Client class.
              http://boto3.readthedocs.io/en/latest/guide/events.html#extensibility-guide

              Alternatives considered:
              https://github.com/boto/boto3/issues/3197#issue-1175578228
              https://github.com/aws-samples/boto-session-manager-project
              """
              if cls._boto3_client is None:
                cls._boto3_client = boto3.client(
                  "backup", config=botocore.config.Config(retries={"mode": "standard"})
                )
              return cls._boto3_client

            # pylint: disable=missing-function-docstring

            @property
            def _from_job_details(self):
              return self._from_event.get("detail", {})

            @property
            def from_event(self):
              return self._from_event

            @property
            def from_job_id(self):
              return self._from_job_details.get(self._from_job_id_key, "")

            @property
            def from_backup_arn(self):  # Reserve from_rsrc_arn for original backups
              return ""

            @property
            def arn(self):
              return self._from_event.get("resources", [""])[0]

            # pylint: enable=missing-function-docstring

            def valid(self):
              """Return True if all required attributes are non-empty

              A cursory validation, but EventBridge filters in CloudFormation allow only
              acceptable events, which come from AWS Backup.
              """
              return all([self.from_job_id, self.arn])  # More attributes coming!

            def log_action(self, action_name, action_kwargs, exception=None, resp=None):
              """Log the AWS Lambda event and the outcome of an action on a backup
              """
              log_level = logging.INFO if boto3_success(resp) else logging.ERROR

              log("LAMBDA_EVENT", self.from_event, log_level)
              log(f"{action_name.upper()}_KWARGS", action_kwargs, log_level)

              if exception is not None:
                log("EXCEPTION", exception, log_level)
              elif resp is not None:
                log("AWS_RESPONSE", resp, log_level)

            def do_action(
              self, action_name, kwargs_add={}, validate_backup=True
            ):  # pylint: disable=dangerous-default-value
              """Take an AWS Backup method and kwargs, log outcome, and return response
              """
              action_kwargs = self.action_kwargs_base.get(
                action_name, self.action_kwargs_base["DEFAULT"]
              ) | kwargs_add  # Copy, don't update!
              resp = None

              if validate_backup and not self.valid():
                self.log_action(action_name, action_kwargs)
              else:
                action_method = getattr(self.get_boto3_client(), action_name)
                try:
                  resp = action_method(**action_kwargs)
                except Exception as misc_exception:
                  self.log_action(action_name, action_kwargs, exception=misc_exception)
                  raise
                self.log_action(action_name, action_kwargs, resp=resp)

              return resp


          class BackupCopy(Backup):
            """AWS Backup recovery point copy, from start_copy_job

            https://docs.aws.amazon.com/aws-backup/latest/devguide/eventbridge.html#copy-job-state-change-completed

            Why didn't AWS use the same structure and keys for start_backup_job and the
            destination half of start_copy_job ? Both methods put a backup in a
            destination vault.
            """
            _from_job_id_key = "copyJobId"

            @property
            def from_backup_arn(self):  # Reserve from_rsrc_arn for original backups
              return self._from_event.get("resources", [""])[0]

            @property
            def arn(self):  # pylint: disable=missing-function-docstring
              return self._from_job_details.get("destinationRecoveryPointArn", "")

            def valid(self):
              """Return True if all required attributes are non-empty

              A cursory validation, but EventBridge filters in CloudFormation allow only
              acceptable events, which come from AWS Backup.
              """
              return all([self.from_job_id, self.from_backup_arn, self.arn])

          def get_update_lifecycle_kwargs(describe_resp):
            """Take a describe response, return update_recovery_point_lifecycle kwargs

            Sets/reduces DeleteAfterDays, so a backup that has been copied to another
            vault can be scheduled for deletion from the original vault. If the result
            dict is empty, no lifecycle update is needed.

            Warnings:
            - Before calling describe_recovery_point , use tzset to set the local time
              zone to UTC, for correct results.
            - For safety, this function works in UTC whole days, stripping time and
              leaving a whole-day margin ( +1 and strict < inequality ). AWS Backup
              measures lifecycles (MoveToColdStorageAfterDays, DeleteAfterDays) in whole
              days, but CreationDate -- misnamed -- includes a precise time, and then
              deletion occurs "at a randomly chosen point over the following 8 hours".
              https://docs.aws.amazon.com/aws-backup/latest/devguide/recov-point-create-on-demand-backup.html
            """
            kwargs_out = {}
            lifecycle = dict(describe_resp.get("Lifecycle", {}))  # Update the copy...

            creation_date = describe_resp["CreationDate"].date()
            today = datetime.date.today()
            days_old = (today - creation_date).days

            delete_after_days_minima = [days_old, 1, NEW_DELETE_AFTER_DAYS]
            delete_after_days_maximum = lifecycle.get("DeleteAfterDays")  # Don't delay

            storage_class = describe_resp.get("StorageClass")
            cold_storage_after_days = (
              lifecycle.get("MoveToColdStorageAfterDays")
              if lifecycle.get("OptInToArchiveForSupportedResources", False) else
              None
            )

            if storage_class == "DELETED":
              delete_after_days_maximum = 0
            elif cold_storage_after_days is not None:
              if (storage_class == "WARM") and (days_old < cold_storage_after_days):
                # Has not yet transitioned cold storage, and is not scheduled to, today
                lifecycle.update({
                  "OptInToArchiveForSupportedResources": False,
                  "MoveToColdStorageAfterDays": -1,
                })
              else:
                # Has already transitioned cold storage, or is scheduled to, today
                delete_after_days_minima.append(cold_storage_after_days + 90)
            elif storage_class == "COLD":
              # In case AWS Backup someday supports creation in/non-scheduled move to
              # cold storage, could have entered cold storage as late as today
              delete_after_days_minima.append(days_old + 90)

            delete_after_days = max(delete_after_days_minima) + 1
            if (
              (delete_after_days_maximum is None)
              or (delete_after_days < delete_after_days_maximum)
            ):
              lifecycle["DeleteAfterDays"] = delete_after_days
              kwargs_out = {"Lifecycle": lifecycle}

            return kwargs_out


          def lambda_handler_copy(event, context):  # pylint: disable=unused-argument
            """Copy a backup to a vault in another AWS account OR another region
            """
            backup = Backup.new(event)
            backup.do_action(
              "start_copy_job",
              {
                "RecoveryPointArn": backup.arn,
                "IdempotencyToken": backup.from_job_id,
              }
            )


          def lambda_handler_update_lifecycle(event, context):  # pylint: disable=unused-argument
            """Schedule deletion of a backup that has been copied to another vault

            Warning:
            - Before calling describe_recovery_point , use tzset to set the local time
              zone to UTC, for correct results.
            """
            backup = Backup.new(event)
            describe_resp = backup.do_action(
              "describe_recovery_point",
              {"RecoveryPointArn": backup.from_backup_arn}
            )
            if boto3_success(describe_resp):
              kwargs_add = get_update_lifecycle_kwargs(describe_resp)
              if kwargs_add:
                backup.do_action(
                  "update_recovery_point_lifecycle",
                  kwargs_add | {"RecoveryPointArn": backup.from_backup_arn},
                  validate_backup=False
                )
        # ZIPFILE_END
